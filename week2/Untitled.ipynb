{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Cost of learning mean of training data: 80.18050338682674\n",
      "Lets see if we can do better with just one question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liushuwen/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/liushuwen/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx, val, left, right 12 7.85 31.983739837398375 18.416370106761565\n",
      "Feature name of idx LSTAT\n",
      "Score of model 53.1384440544838\n",
      "lets compare with sklearn decision tree\n",
      "dc score 53.13844405448379\n",
      "feature names - for comparison [(0, 'CRIM'), (1, 'ZN'), (2, 'INDUS'), (3, 'CHAS'), (4, 'NOX'), (5, 'RM'), (6, 'AGE'), (7, 'DIS'), (8, 'RAD'), (9, 'TAX'), (10, 'PTRATIO'), (11, 'B'), (12, 'LSTAT')]\n",
      "exporting tree to dtree.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pdb \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from io import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "def plot_tree(dtree, feature_names):\n",
    "    \"\"\" helper function \"\"\"\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names=feature_names)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    print('exporting tree to dtree.png')\n",
    "    graph.write_png('dtree.png')\n",
    "\n",
    "\n",
    "class RegressionStump():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" The state variables of a stump\"\"\"\n",
    "        self.idx = None\n",
    "        self.val = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def fit(self, data, targets):\n",
    "        \"\"\" Fit a decision stump to data\n",
    "        \n",
    "        Find the best way to split the data in feat  minimizig the cost (0-1) loss of the tree after the split \n",
    "    \n",
    "        Args:\n",
    "           data: np.array (n, d)  features\n",
    "           targets: np.array (n, ) targets\n",
    "    \n",
    "        sets self.idx, self.val, self.left, self.right\n",
    "        \"\"\"\n",
    "        # update these three\n",
    "        self.idx = 0\n",
    "        self.val = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        ### YOUR CODE HERE\n",
    "        cost = np.zeros((data.shape[0], data.shape[1]))\n",
    "        left_mean = np.zeros((data.shape[0], data.shape[1]))\n",
    "        right_mean = np.zeros((data.shape[0], data.shape[1]))\n",
    "        \n",
    "        for d in range(data.shape[1]):\n",
    "            for i in range(data.shape[0]):\n",
    "                val = data[i,d]\n",
    "                left = targets[data[:,d] <= val]\n",
    "                right = targets[data[:,d] > val]\n",
    "                left_mean[i,d] = np.mean(left)\n",
    "                right_mean[i,d] = np.mean(right)\n",
    "                \n",
    "                cost[i,d] = np.sum((left-np.mean(left))**2)+np.sum((right-np.mean(right))**2)\n",
    "                \n",
    "        argmin = np.unravel_index(np.argmin(cost, axis=None), cost.shape)        \n",
    "        \n",
    "        self.idx = argmin[1]\n",
    "        self.val = data[argmin]\n",
    "        self.left = left_mean[argmin]\n",
    "        self.right = right_mean[argmin]\n",
    "        \n",
    "        ### END CODE\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Regression tree prediction algorithm\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "        \n",
    "        returns pred: np.array shape n,  model prediction on X\n",
    "        \"\"\"\n",
    "        pred = np.zeros((X.shape[0],))\n",
    "        ### YOUR CODE HERE\n",
    "        left_selector = X[:,self.idx] <= self.val\n",
    "        right_selector = X[:,self.idx] > self.val\n",
    "        \n",
    "        pred[left_selector] = self.left\n",
    "        pred[right_selector] = self.right\n",
    "        \n",
    "        ### END CODE\n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute accuracy of model\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "            y: np.array, shape n, \n",
    "\n",
    "        returns out: scalar - means least scores cost\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        out = np.mean((self.predict(X)-y)**2)\n",
    "        \n",
    "        ### END CODE\n",
    "        return out\n",
    "        \n",
    "\n",
    "### YOUR CODE HERE\n",
    "### END CODE\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Simple method testing \"\"\"\n",
    "    boston = load_boston()\n",
    "    # split 80/20 train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
    "                                                        boston.target,\n",
    "                                                        test_size=0.2)\n",
    "\n",
    "    baseline_accuracy = np.mean((y_test-np.mean(y_train))**2)\n",
    "    print('Least Squares Cost of learning mean of training data:', baseline_accuracy) \n",
    "    print('Lets see if we can do better with just one question')\n",
    "    D = RegressionStump()\n",
    "    D.fit(X_train, y_train)\n",
    "    print('idx, val, left, right', D.idx, D.val, D.left, D.right)\n",
    "    print('Feature name of idx', boston.feature_names[D.idx])\n",
    "    print('Score of model', D.score(X_test, y_test))\n",
    "    print('lets compare with sklearn decision tree')\n",
    "    dc = DecisionTreeRegressor(max_depth=1)\n",
    "    dc.fit(X_train, y_train)\n",
    "    dc_score = ((dc.predict(X_test)-y_test)**2).mean()\n",
    "    print('dc score', dc_score)\n",
    "    print('feature names - for comparison', list(enumerate(boston.feature_names)))\n",
    "    plot_tree(dc, boston.feature_names)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
